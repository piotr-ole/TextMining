{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czyszczenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def clean_and_save(path_rawfile, path_direction):\n",
    "    f = codecs.open(path_rawfile, encoding='utf-8')\n",
    "    l = list()\n",
    "    for line in f:\n",
    "        l.append(line)\n",
    "    clean_document = re.sub( '<E(.{1,100})\">', '' , l[0]) # ten przedzial dlatego, zeby usuwalo kolejne fragmenty, a nie duzy fragment tekstu\n",
    "    clean_document = re.sub('</Entity>', '' , clean_document)\n",
    "    f.close()\n",
    "    f = codecs.open(path_direction, \"w\", encoding = 'utf-8')\n",
    "    f.write(clean_document)\n",
    "    f.close()\n",
    "\n",
    "def clean_all_files(absolute_path_source, absolute_path_direction, dirs_list):\n",
    "    for dirc in dirs_list:\n",
    "        path_source = absolute_path_source + '\\\\' + dirc\n",
    "        path_direction = absolute_path_direction + '\\\\' + dirc\n",
    "        for file in os.listdir(path_source):\n",
    "            clean_and_save(path_source + '\\\\' + file, path_direction + '\\\\' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_path_source = 'C:\\\\Users\\\\piotr\\\\Desktop\\\\text\\\\TextMining\\\\learningData'\n",
    "absolute_path_direction = 'C:\\\\Users\\\\piotr\\\\Desktop\\\\text\\\\TextMining\\\\cleanData'\n",
    "dirs = ['korpusGAZETA', 'korpusONET']\n",
    "clean_all_files(absolute_path_source, absolute_path_direction, dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from IPython.core.debugger import set_trace\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "def flatten_list(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def read_document(file):\n",
    "    with codecs.open(file, 'r', encoding = 'UTF-8') as f:\n",
    "        document = f.read()\n",
    "    return document\n",
    "\n",
    "def find_entity(doc, entity_forms):\n",
    "    # For example entity_forms = [ 'Donald Tusk', 'Donalda Tuska'] itp.\n",
    "    positions = []\n",
    "    for form in entity_forms:\n",
    "        positions.append((m.start(), m.end()) for m in re.finditer(form, doc))\n",
    "    result = flatten_list(positions) \n",
    "    result.sort()\n",
    "    return result\n",
    "\n",
    "def tag_template(entity_base_form, type_ , category):\n",
    "    tag_start = '<Entity name=\"' + entity_base_form + '\" type=\"' + type_ + '\" category=\"' + category + '\">'\n",
    "    tag_end = '</Entity>'\n",
    "    return (tag_start, tag_end)\n",
    "\n",
    "def slice_string_and_tag(string, slice_points, tag):\n",
    "    l = []\n",
    "    start = 0\n",
    "    for slice_start, slice_end in slice_points:\n",
    "        l.append(string[start:slice_start])\n",
    "        l.append(tag[0])\n",
    "        l.append(string[slice_start:slice_end])\n",
    "        l.append(tag[1])\n",
    "        start = slice_end\n",
    "    l.append(string[start:-1])\n",
    "    return ''.join(l)\n",
    "        \n",
    "    \n",
    "def tag_document_base(file, entity_base_form, entity_forms, type_ , category):\n",
    "    # This one if we search for people\n",
    "    doc = read_document(file)\n",
    "    positions = find_entity(doc, entity_forms)\n",
    "    tag = tag_template(entity_base_form, type_ , category)\n",
    "    return slice_string_and_tag(doc, positions, tag)\n",
    "    \n",
    "def tag_document_model(file, positions, entity_base_form, type_ , category):\n",
    "    # This one propably better for model (I assume it gives us starting and ending positions)\n",
    "    doc = read_document(file)\n",
    "    tag = tag_template(entity_base_form, type_ , category)\n",
    "    return slice_string_and_tag(doc, positions, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Niesamowita historia książki ze specjalną dedykacją, którą <Entity name=\"Marek Edelman\" type=\"person\" category=\"lekarz\">Marek Edelman</Entity>, legendarny przywódca powstania w getcie warszawskim, podarował prezydentowi Kielc Wojciechowi Lubawskiemu. Odnaleziono ją na śmietniku na osiedlu Szydłówek - W sobotę późnym popołudniem wracaliśmy do swojego mieszkania na osiedlu Szydłówek w Kielcach. Kiedy wysiedliśmy z samochodu, moja dziewczyna Katarzyna zauważyła, że przed śmietnikiem leżą książki - opowiada kielczanin Jakub Kotula. Podeszła, przejrzała i dwie zabrała ze sobą. - Jedna z nich to była książka <Entity name=\"Marek Edelman\" type=\"person\" category=\"lekarz\">Marka Edelmana</Entity> \"I była miłość w getcie\". Katarzyna nie mogła uwierzyć, że ktoś mógł wyrzucić ją na śmietnik - mówi. Ich zdziwienie było jeszcze większe, gdy zobaczyli dedykację: \"Panu Prezydentowi Wojciechowi Lubawskiemu z pełnym uznaniem i pozdrowieniami\". A pod spodem podpis <Entity name=\"Marek Edelman\" type=\"person\" category=\"lekarz\">Marka Edelmana</Entity> i data 19 kwietnia 2009 roku. Jedna z ostatnich dedykacji Legendarny przywódca powstania w getcie warszawskim zmarł pół roku później, 2 października 2009 roku. Kuba i Kasia zastanawiali się, czy autorem dedykacji jest <Entity name=\"Marek Edelman\" type=\"person\" category=\"lekarz\">Marek Edelman</Entity>. - Sprawdzaliśmy w internecie, miał podobny charakter pisma. Pozostaje tylko pytanie, dlaczego ktoś chciał się jej pozbyć? - zastanawia się Jakub Kotula. Jak się okazuje, to była jedna z ostatnich książek, którą Edelman komuś dedykował. - To nie był człowiek, który siadał przy stoliku i podpisywał książki. Nie dbał o tego typu sprawy - przyznaje Bogdan Białek, prezes Stowarzyszenia im. Jana Karskiego. Dodaje, że w ostatnich latach życia Edelman miał bardzo dobre zdanie o Kielcach i kielczanach. A nie zawsze tak było. W naszym mieście był zaledwie dwa razy. Już nie widział tu śladów zła Pierwszy raz przyjechał tuż po pogromie. Zabrał rannych i przetransportował ich do szpitala w Łodzi. - Później powtarzał, że Kielce to ponure miasto, kojarzyło się doktorowi ze złem. Zapraszałem go wielokrotnie, ale zawsze odmawiał - mówi Białek. Zgodził się dopiero jesienią 2004 roku, kiedy szkole przy ul. Jasnej nadawano imię Jana Karskiego. - Wtedy komplementował miasto i ludzi, którzy tu mieszkają. Powiedział do młodzieży w KCK, że Kielce się zmieniły, że nie widzi tu śladów zła - podkreśla Bogdan Białek. - Był bardzo miłym i bezpośrednim człowiekiem. Zapamiętałam z tej wizyty, że zaprosiłam go z Bogdanem Białkiem do gabinetu, ale wybrał foyer. Poczęstowałam ich wtedy koniakiem - opowiada Magdalena Kusztal, dyrektorka KCK. Dodaje, że później Edelman podarował jej książkę z dedykacją. - Napisał w niej: \"Na pamiątkę dobrze smakującego koniaczku\" - mówi. Prezes Stowarzyszenia im. Jana Karskiego wspomina, że Edelman bardzo chciał przyjechać na uroczystą premierę \"Listów z placu Zgody\". - Już się samodzielnie nie poruszał, siedział na wózku inwalidzkim, a mimo to dawał mi do końca nadzieję, że do nas przyjedzie. Choć ja nie miałbym sumienia go tu ściągać, był już w złym stanie - twierdzo Białek. Prezydent: ktoś książkę ukradł Teraz już wiadomo, że to właśnie wtedy, podczas uroczystej premiery \"Listów\", która odbyła się pod koniec kwietnia 2009 roku w KCK, on wręczył Wojciechowi Lubawskiemu ten znaleziony teraz na śmietniku egzemplarz \"I była miłość w getcie\". - Dobrze to pamiętam. Odłożyłem na moment książkę, a po premierze okazało się, że jej już nie ma. Ktoś po prostu z premedytacją ukradł - mówi prezydent. Dodaje, że szukał później książki, dopytywał woźnych, czy jej nie widzieli. Bezskutecznie. Wojciech Lubawski podkreśla, że była dla niego bardzo ważna. - Miała wartość sentymentalną. Jeśli teraz by się udało ją odzyskać, byłbym bardzo szczęśliwy. A ludziom, którzy ją znaleźli, kupię piękny prezent - zapowiada prezydent'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_document_base('doc2', 'Marek Edelman', ['Marek Edelman', 'Marka Edelmana'], type_ = 'person', category='lekarz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzanie zawodów w Wikipedii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Słownik z zawodami (zrobiony recznie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def read_jobs(file):\n",
    "    l = []\n",
    "    with codecs.open(file , \"r\", encoding = 'UTF-8') as f:\n",
    "        l = f.read().split('\\n')\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'podzawody'\n",
    "jobs = dict()\n",
    "keys = read_jobs('zawody.txt')\n",
    "for key in keys:\n",
    "    jobs[key] = set(read_jobs(folder + '/' + key + '.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morfeusz + wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import morfeusz2\n",
    "wikipedia.set_lang(\"pl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_job_for_person(person):\n",
    "    # looks for one of jobs in wikipedia summary page\n",
    "    global jobs\n",
    "    profession = []\n",
    "    morf = morfeusz2.Morfeusz()\n",
    "    try:\n",
    "        page = wikipedia.page(person)\n",
    "        l = []\n",
    "        analysis = morf.analyse(page.summary)\n",
    "        for i, j, interp in analysis:\n",
    "            # Interp[0] - word in page.summary\n",
    "            # Interp[1] - The base form of Interp[0] (with some tags or clear)\n",
    "            #print(interp) #uncomment to see\n",
    "            l.append(interp[0])\n",
    "            l.append(interp[1])\n",
    "        simple_words = set(l)\n",
    "        for key in jobs.keys():\n",
    "            if jobs[key].intersection(simple_words) != set():\n",
    "                profession.append(key)\n",
    "        if profession == []:\n",
    "            profession.append('None')\n",
    "    except:\n",
    "        profession.append('None')\n",
    "    return profession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roman Giertych : ['politycy', 'prawnicy']\n",
      "Donald Tusk : ['politycy']\n",
      "Jarosław Kaczyński : ['politycy', 'prawnicy']\n",
      "Janusz Korwin-Mikke : ['politycy', 'dziennikarze']\n",
      "Barbara Kurdej-Szatan : ['aktorzy', 'muzycy']\n",
      "Robert Mazurek : ['dziennikarze']\n",
      "Roman Dmowski : ['politycy', 'dziennikarze']\n",
      "Tomasz Kłos : ['sportowcy']\n",
      "Andrzej Grabowski : ['aktorzy', 'muzycy']\n",
      "Juliusz Słowacki : ['poeci']\n"
     ]
    }
   ],
   "source": [
    "people = ['Roman Giertych', 'Donald Tusk', 'Jarosław Kaczyński', 'Janusz Korwin-Mikke',\n",
    "         'Barbara Kurdej-Szatan', 'Robert Mazurek', 'Roman Dmowski', 'Tomasz Kłos',\n",
    "         'Andrzej Grabowski', 'Juliusz Słowacki']\n",
    "for human in people:\n",
    "    print(human + ' : ' + str(exclude_job_for_person(human)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Proby uzyskania podstawowej wersji wyrażenia (nieudane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = ['Kasi Kowalskiej', 'Roberta Lewandowskiego', 'Ania Lewandowska', 'Donaldem Tuskiem', 'Januszem Korwin-Mikke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Kasia'}\n",
      "{'Kasia'}\n",
      "{'Kasia'}\n",
      "{'Kasia'}\n",
      "{'Kasia'}\n",
      "{'Kasia'}\n",
      "{'Kasia', 'Kowalska'}\n",
      "{'Kasia', 'Kowalska'}\n",
      "{'Robert:s2'}\n",
      "{'Robert:s2'}\n",
      "{'Robert:s2', 'Roberta'}\n",
      "{'Robert:s2', 'Robert:s1', 'Roberta'}\n",
      "{'Robert:s2', 'Robert:s1', 'Roberta', 'Roberto'}\n",
      "{'Roberto', 'Lewandowski', 'Robert:s2', 'Roberta', 'Robert:s1'}\n",
      "{'Ania'}\n",
      "{'Ania', 'Lewandowska'}\n",
      "{'Ania', 'Lewandowska'}\n",
      "{'Donald'}\n",
      "{'Donald', 'Tusk:s1'}\n",
      "{'Janusz:s2'}\n",
      "{'Janusz:s2', 'Janusz:s1'}\n",
      "{'Janusz:s2', 'Janusz:s1', 'Korwin:s1'}\n",
      "{'Janusz:s2', 'Janusz:s1', 'Korwin:s1'}\n",
      "{'Janusz:s2', 'Korwin:s2', 'Janusz:s1', 'Korwin:s1'}\n",
      "{'Janusz:s2', 'Korwin:s2', 'Janusz:s1', 'Korwin:s1', 'Korwin:s3'}\n",
      "{'Janusz:s2', 'Korwin:s2', 'Janusz:s1', 'Korwin:s1', 'Korwin:s3'}\n",
      "{'Janusz:s2', 'Korwin:s2', 'Janusz:s1', 'Korwin:s1', 'Korwin:s3'}\n",
      "{'Mikke:s1', 'Janusz:s2', 'Korwin:s2', 'Janusz:s1', 'Korwin:s1', 'Korwin:s3'}\n",
      "{'Mikke:s1', 'Janusz:s2', 'Korwin:s2', 'Janusz:s1', 'Korwin:s1', 'Korwin:s3'}\n",
      "{'Mikke:s1', 'Janusz:s2', 'Korwin:s2', 'Janusz:s1', 'Mikke:s2', 'Korwin:s1', 'Korwin:s3'}\n",
      "{'Mikke:s1', 'Janusz:s2', 'Korwin:s2', 'Janusz:s1', 'Mikke:s2', 'Korwin:s1', 'Korwin:s3'}\n"
     ]
    }
   ],
   "source": [
    "person = 'Kasi Kowalskiej'\n",
    "for person in persons:\n",
    "    morf = morfeusz2.Morfeusz()\n",
    "    #page = wikipedia.page(person)\n",
    "    analysis = morf.analyse(person)\n",
    "    l = []\n",
    "    for i, j, interp in analysis:\n",
    "        if interp[2].split(':')[0] == 'subst':# or interp[2].strip(':')[0] == 'subst':\n",
    "            l.append(interp[1])\n",
    "        print(set(l))\n",
    "    l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kasi', 'Kasia', 'subst:sg:gen:f', ['imię'], [])\n",
      "('Kasi', 'Kasia', 'subst:sg:dat.loc:f', ['imię'], [])\n",
      "('Kasi', 'Kasia', 'subst:pl:gen:f', ['imię'], ['hom.', 'rzad.'])\n",
      "('Kowalskiej', 'kowalski', 'adj:sg:dat:f:pos', [], [])\n",
      "('Kowalskiej', 'kowalski', 'adj:sg:gen:f:pos', [], [])\n",
      "('Kowalskiej', 'kowalski', 'adj:sg:loc:f:pos', [], [])\n",
      "('Kowalskiej', 'Kowalska', 'subst:sg:gen:f', ['nazwisko'], [])\n",
      "('Kowalskiej', 'Kowalska', 'subst:sg:dat.loc:f', ['nazwisko'], [])\n"
     ]
    }
   ],
   "source": [
    "person = 'Kasi Kowalskiej'\n",
    "morf = morfeusz2.Morfeusz()\n",
    "#page = wikipedia.page(person)\n",
    "analysis = morf.analyse(person)\n",
    "l = []\n",
    "for i, j, interp in analysis:\n",
    "    print(interp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysis",
   "language": "python",
   "name": "dataanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
